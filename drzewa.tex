\subsection{Wzmacniane drzewa decyzyjne}
\label{subsec:drzewa}

Wzmacniane drzewa decyzyjne są jednym z rozwinięć klasycznego algorytmu drzewa decyzyjnego. 
Pojedyncze drzewo decyzyjne dzieli przestrzeń cech uczących przy pomocy prostopadłych cięć, na mniejsze/większe niż zadana wartość w przypadku zmiennej ciągłej lub na należące/nie należące do danej klasy w przypadku zmiennej kategorycznej. Każdy podział, nazywany węzłem, daje dwie gałęzie, które można dalej niezależnie dzielić aż do ostatniego poziomu (liści). Kolejne podziały wybierane są tak, aby zbiory przykładów wpadające do poszczególnych gałęzi były jak najbardziej jednorodne. Stosuje się różne miary nieporządku takie tak: indeks Gini $G_L$ lub entropia $S_L$ \footnote{$G_L = 1 - \sum\limits_k p_k^2$ oraz $S_L = \sum\limits_k -p_k \log p_k$ , gdzie $p_k$ to stosunek liczby przypadków klasy $k$ do liczby wszystkich przypadków w liściu $L$}.

Drzewa decyzyjne są często łączone w komitety klasyfikatorów \angterm{ensemble methods}. Wiele "słabych" klasyfikatorów jest łączonych w jeden "silny" na dwa sposoby: workowania \angterm{bagging} \cite{breiman1996bagging} oraz wzmacniania \angterm{boosting} \cite{freund1997decision}, które są często ze sobą porównywane.

\textit{Bagging} -- w zastosowaniu dla drzew decyzyjnych nazywany algorytmem lasów losowych \angterm{random forest} - polega na wytrenowaniu wielu drzew, każdego na podstawie $N$ przykładów losowo wylosowanych z powtórzeniami spośród $N$-licznego zbioru treningowego. Dodatkowo, do uczenia każdego drzewa używa się tylko podzbioru wszystkich cech uczących. Końcową predykcję algorytmu otrzymuje się poprzez "głosowanie" wszystkich drzew z odpowiednimi wagami.

\textit{Boosting} -- wzmacniane drzewa decyzyjne \angterm{boosted decision trees} -- jest metodą podobną do \textit{baggingu}. Główną różnicą jest zwiększanie wag przykładom uczącym, które przez poprzednie drzewo zostały źle zaklasyfikowane -- każde kolejne drzewo koncentruje się bardziej na poprawie błędów poprzednich drzew. Widać tu kolejną ważną cechę odróżniającą obie metody: \textit{boosting} jest algorytmem sekwencyjnym podczas gdy \textit{bagging} daje się trywialnie zrównoleglić (każde drzewo trenowane jest w osobnym wątku).



\subsubsection*{Parametry i sposób trenowania drzew decyzyjnych na analizowanych danych}

W niniejszej pracy wykorzystano wzmacniane drzewa decyzyjne zaimplementowane w wydajnej bibliotece \texttt{XGBoost} \cite{chen16}. Szybkość obliczeń jest bardzo ważna, gdyż oprócz komfortu pracy z algorytmem, przekłada się
na jakość otrzymanych wyników -- krótszy czas obliczeń oznacza możliwość przeprowadzenia większej ilości eksperymentów i lepsze dobranie parametrów oraz danych.
Implementacja wzmacnianych drzew decyzyjnych w \texttt{XGBoost} wykorzystuje wszystkie rdzenie procesora, pomimo że sam algorytm ma charakter sekwencyjny -- jest to możliwe dzięki paralelizacji procesu tworzenia każdego drzewa (przed każdym podziałem konieczne jest sprawdzenie pewnej ilości możliwych zmiennych i wartości progowych i ten proces jest wykonywany równolegle).

Dzięki szybkiemu uczeniu się algorytmu, możliwe było użycie kosztownego obliczeniowo automatycznego przeszukiwania przestrzeni parametrów przy pomocy przeszukiwania losowego \angterm{random search}, które jest zwykle preferowane nad przeszukiwanie sieciowe \cite{bergstra12}. W tym celu cały zbiór danych dzielony był na dwie części: trenującą oraz testową (80/20\%). Następnie algorytm był trenowany i oceniany z użyciem trzy- lub pięciokrotnej walidacji krzyżowej \angterm{cross-validation} na zbiorze trenującym dla różnych zestawów parametrów. Model z najlepszym wynikiem uzyskanym w walidacji krzyżowej był sprawdzany na zbiorze testowym.

Parametry optymalizowane w opisanym procesie to:
\begin{itemize}
	\item \textit{max\_depth} -- maksymalna głębokość każdego drzewa (niekoniecznie osiągana)
	\item \textit{n\_estimators} -- liczba drzew
	\item \textit{learning\_rate} -- parametr szybkości uczenia, komplementarny do \textit{n\_estimators}, w praktyce można ustalić liczbę drzew i szukać optymalnej szybkości uczenia
	\item \textit{subsample, colsample\_bytree, colsample\_bylevel} -- parametry regularyzacyjne określające ułamek kolejno: danych użytych do trenowania każdego drzewa, kolumn użytych w każdym drzewie (cechy losowane raz dla danego drzewa), kolumn użytych przy każdym podziale (cechy losowane przy każdym podziale)
	 \item $\gamma$ -- minimalny zysk w postaci zmniejszenia wartości funkcji straty konieczny do wykonania podziału
\end{itemize}

